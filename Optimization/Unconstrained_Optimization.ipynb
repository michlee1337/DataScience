{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implement gradient descent here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(x,y):\n",
    "    # Function definition here\n",
    "    return (1-x)**2 + 100*(y-x**2)**2\n",
    "\n",
    "def gradf(x,y):\n",
    "    # Gradient definition here\n",
    "    return np.array([2*(x-1) + 200*(y - x**2)*(-2*x), 200*(y-x**2)])\n",
    "\n",
    "def hessian(x,y):\n",
    "    # Gradient definition here\n",
    "    return np.array([[2+200*(6*x**2 - 2*y), -400*x],\n",
    "                     [-400*x, 200]])\n",
    "\n",
    "def exactStep(f,df,step,x0):\n",
    "    X = x0\n",
    "    for i in range(0,1000):\n",
    "        X_new = X - step*gradf(X[0], X[1])\n",
    "        if np.linalg.norm(df(X_new[0],X_new[1])) <= 1e-6:\n",
    "            return(\"Root at: {}\".format(X_new))\n",
    "        X = X_new\n",
    "        print(X)\n",
    "    return(\"Root not reached. Terminated at {}.\".format(X))\n",
    "\n",
    "def bisectionSearch(f,df,step,x0):\n",
    "    X = x0\n",
    "    for i in range(0,1000):\n",
    "        if np.linalg.norm(df(X[0],X[1])) <= 1e-6:\n",
    "            return(\"Root at: {}\".format(X))\n",
    "        elif np.linalg.norm(df(X[0],X[1])) < 0:\n",
    "            X = X+step\n",
    "        elif np.linalg.norm(df(X[0],X[1])) > 0:\n",
    "            X = X - step*gradf(X[0], X[1])\n",
    "        print(X)\n",
    "    return(\"Root not reached. Terminated at {}.\".format(X))\n",
    "\n",
    "def newtonStep(f,df,step,x0):\n",
    "    X = x0\n",
    "    for i in range(0,1000):\n",
    "        H_inv = np.linalg.inv(hessian(X[0], X[1]))\n",
    "        grad=gradf(X[0], X[1])\n",
    "        X_new = X - np.matmul(H_inv,grad) #step*gradf(X[0], X[1])\n",
    "        print(X_new)\n",
    "        if np.linalg.norm(np.array(X) - np.array(X_new)) <= 1e-6:\n",
    "            return(\"Root at: {}\".format(X_new))\n",
    "        X = X_new\n",
    "    return(\"Root not reached. Terminated at {}.\".format(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[-0.82535211  0.68056338]\n",
      "(2,)\n",
      "[ 0.79208505 -1.98870424]\n",
      "(2,)\n",
      "[0.79248166 0.62802703]\n",
      "(2,)\n",
      "[0.99999347 0.95692579]\n",
      "(2,)\n",
      "[0.99999415 0.9999883 ]\n",
      "(2,)\n",
      "[1. 1.]\n",
      "(2,)\n",
      "[1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Root at: [1. 1.]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = np.array([-0.8,1]) # Starting state\n",
    "step = 0.001\n",
    "newtonStep(f,gradf,step,X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(f, df, d, X):\n",
    "    r = d - gradf(*X) \n",
    "    return((r.dot(r))/(d.dot(f(*d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjGradient(f,df,x0,n):\n",
    "    X = x0\n",
    "    # take negative gradient as first direction vector\n",
    "    d = [None for _ in range(n+1)]\n",
    "    d[0] = np.array(-gradf(X[0],X[1]))\n",
    "    alphas = [None for _ in range(n+1)]\n",
    "    for i in range(n):\n",
    "        # remainder = b - Ax\n",
    "        r = d[i] - gradf(*X) \n",
    "\n",
    "        a = (r.dot(r))/(d[i].dot(f(*d[i])))\n",
    "        alphas[i] = a\n",
    "\n",
    "        X_new = X + a.dot(d[i])\n",
    "\n",
    "        r_new = r.dot(f(*d[i]))\n",
    "\n",
    "        beta = (r_new.dot(r_new))/(r.dot(r))\n",
    "\n",
    "        d[i+1] = r_new + beta*(d[i])\n",
    "    r_n = d[n] - gradf(*X) \n",
    "    alphas[n] = (r.dot(r))/(d[n].dot(f(*d[n])))\n",
    "    return(d, alphas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([-111.6,  -72. ]),\n",
       "  array([-2.74784473e+22, -1.77280305e+22]),\n",
       "  array([-8.93160882e+205, -5.76232827e+205])],\n",
       " [array([-4.02897828e-08, -6.24491634e-08]),\n",
       "  array([-6.82587252e-70, -1.05801024e-69]),\n",
       "  array([-0., -0.])])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtonStep(f,gradf,np.array([-0.8,1]),2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
